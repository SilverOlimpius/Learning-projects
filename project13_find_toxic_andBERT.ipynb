{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект 13 создание модели для поиска токсичных комментариев\n",
    "\n",
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "### Инструкция по выполнению проекта\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "### Описание данных\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак.\n",
    "\n",
    "# 1. Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pip\n",
    "#pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import torch\n",
    "#import transformers\n",
    "#from tqdm import notebook\n",
    "\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import re\n",
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "from nltk.stem import SnowballStemmer \n",
    "english_stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('toxic_comments.csv')\n",
    "data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "\n",
    "#временная мера для уменьшения данных:\n",
    "#data = data.sample(10000).reset_index(drop=True)\n",
    "\n",
    "#Приведём данные в более единый формат, методом приведения к нижнему регистру:\n",
    "data['text'] = data['text'].str.lower()\n",
    "\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lemmatize_and_clear_text(data,column_name):\n",
    "    lemmatize_and_clear_text_data = []\n",
    "    for index in data.index:\n",
    "        row_text = data.loc[index,column_name]\n",
    "    \n",
    "        re_sub = re.sub(r'[^a-zA-Z ]', ' ', row_text)\n",
    "        re_sub_split = re_sub.split()\n",
    "        re_sub_split_text = \" \".join(re_sub_split)\n",
    "        \n",
    "        lemm_list = english_stemmer.stem(re_sub_split_text)\n",
    "        #lemm_list = m.lemmatize(re_sub_split_text) #pymystem3\n",
    "        lemm_text = \"\".join(lemm_list)\n",
    "        \n",
    "        lemmatize_and_clear_text_data.append(lemm_text)\n",
    "    return lemmatize_and_clear_text_data\n",
    "#\n",
    "data['lemm_text'] = add_lemmatize_and_clear_text(data,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выборочно сократим датасет, чтобы уровнять пропорции токсичных и нетоксичных сообщений\n",
    "\n",
    "data_toxic = data.loc[data['toxic']==1]\n",
    "\n",
    "data_nan_toxic = data.loc[data['toxic']==0]\n",
    "data_nan_toxic = data_nan_toxic.sample(len(data_toxic)).reset_index(drop=True)\n",
    "\n",
    "data_proportional = pd.concat([data_toxic, data_nan_toxic])\n",
    "data_proportional = data_proportional.sample(frac=1).reset_index(drop=True) \n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>lemm_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>go f yourself\\n\\nyou are so typical of your ty...</td>\n",
       "      <td>1</td>\n",
       "      <td>go f yourself you are so typical of your type ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>hino contessa \\n\\nthe article refers to the co...</td>\n",
       "      <td>0</td>\n",
       "      <td>hino contessa the article refers to the contes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the basis would be the 18 ju 86s operated by t...</td>\n",
       "      <td>0</td>\n",
       "      <td>the basis would be the ju s operated by the so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>dare send messages like this. you are the caus...</td>\n",
       "      <td>1</td>\n",
       "      <td>dare send messages like this you are the cause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"\\nback to bragging i see.  you don't even hav...</td>\n",
       "      <td>1</td>\n",
       "      <td>back to bragging i see you don t even have a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32445</td>\n",
       "      <td>\"\\n\\ncomley stop accusing me of being a vandly...</td>\n",
       "      <td>1</td>\n",
       "      <td>comley stop accusing me of being a vandly pak ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32446</td>\n",
       "      <td>well, better. i will help improve your userpag...</td>\n",
       "      <td>0</td>\n",
       "      <td>well better i will help improve your userpages...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32447</td>\n",
       "      <td>you're an 7åss fùck8 \\n\\nyou're an 7åss fùck8\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>you re an ss f ck you re an ss f ck go sh t yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32448</td>\n",
       "      <td>\"so besides being brainwashed by a cult what i...</td>\n",
       "      <td>0</td>\n",
       "      <td>so besides being brainwashed by a cult what is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32449</td>\n",
       "      <td>you are an utter fucking faggot...u need a lif...</td>\n",
       "      <td>1</td>\n",
       "      <td>you are an utter fucking faggot u need a life ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32450 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  toxic  \\\n",
       "0      go f yourself\\n\\nyou are so typical of your ty...      1   \n",
       "1      hino contessa \\n\\nthe article refers to the co...      0   \n",
       "2      the basis would be the 18 ju 86s operated by t...      0   \n",
       "3      dare send messages like this. you are the caus...      1   \n",
       "4      \"\\nback to bragging i see.  you don't even hav...      1   \n",
       "...                                                  ...    ...   \n",
       "32445  \"\\n\\ncomley stop accusing me of being a vandly...      1   \n",
       "32446  well, better. i will help improve your userpag...      0   \n",
       "32447  you're an 7åss fùck8 \\n\\nyou're an 7åss fùck8\\...      1   \n",
       "32448  \"so besides being brainwashed by a cult what i...      0   \n",
       "32449  you are an utter fucking faggot...u need a lif...      1   \n",
       "\n",
       "                                               lemm_text  \n",
       "0      go f yourself you are so typical of your type ...  \n",
       "1      hino contessa the article refers to the contes...  \n",
       "2      the basis would be the ju s operated by the so...  \n",
       "3      dare send messages like this you are the cause...  \n",
       "4      back to bragging i see you don t even have a c...  \n",
       "...                                                  ...  \n",
       "32445  comley stop accusing me of being a vandly pak ...  \n",
       "32446  well better i will help improve your userpages...  \n",
       "32447  you re an ss f ck you re an ss f ck go sh t yo...  \n",
       "32448  so besides being brainwashed by a cult what is...  \n",
       "32449  you are an utter fucking faggot u need a life ...  \n",
       "\n",
       "[32450 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_proportional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комментарий наставника\n",
    "<span style=\"color:orange\">Хорошо, что борешься с дисбалансом в датасете, но для задач обработки естественного языка требуется как можно больший объем данных для успешного обучения модели, поэтому будь осторожен при избавлении от такого количества ценных данных. Из 159571 записи осталось только 32450.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df_train, df_valid_and_test = train_test_split(data_proportional, test_size=0.4, random_state=12345)\n",
    "df_test, df_valid = train_test_split(df_valid_and_test, test_size=0.5, random_state=12345)\n",
    "\n",
    "train_corpus = df_train['lemm_text'].values.astype('U')\n",
    "train_target = df_train['toxic']\n",
    "\n",
    "valid_corpus = df_valid['lemm_text'].values.astype('U')\n",
    "valid_target = df_valid['toxic']\n",
    "\n",
    "test_corpus = df_test['lemm_text'].values.astype('U')\n",
    "test_target = df_test['toxic']\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)\n",
    "count_tf_idf.fit(train_corpus)\n",
    "\n",
    "train_tf_idf = count_tf_idf.transform(train_corpus)\n",
    "\n",
    "valid_tf_idf = count_tf_idf.transform(valid_corpus)\n",
    "\n",
    "test_tf_idf = count_tf_idf.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комментарий наставника\n",
    "<span style=\"color:green\">Векторизация проведена абсолютно верно.</span> \\\n",
    "<span style=\"color:orange\">Для задач NLP можешь оставлять намного меньше данных под валидацию и тест (можно даже по 5% на каждую выборку). Тренировочная выборка важнее.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.884493670886076\n",
      "CPU times: user 2.77 s, sys: 2.05 s, total: 4.82 s\n",
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ModelLogisticRegression = LogisticRegression(random_state=0)\n",
    "ModelLogisticRegression.fit(train_tf_idf, train_target)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# на валидационных данных F1 = :\n",
    "predictions = ModelLogisticRegression.predict(valid_tf_idf)\n",
    "print(f1_score(predictions, valid_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Комментарий наставника\n",
    "<span style=\"color:orange\">Будь осторожен: по документации sklearn первым параметром в функции метрик всегда подаются реальные (целевые) значения, а только потом уже предсказания. Для F1 последовательность роли не играет, но для других метрик она может быть важна.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8814153649984061\n"
     ]
    }
   ],
   "source": [
    "# на тестовых данных F1 = :\n",
    "predictions = ModelLogisticRegression.predict(test_tf_idf)\n",
    "print(f1_score(predictions, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8068484042553193\n",
      "CPU times: user 6.14 s, sys: 15.2 ms, total: 6.16 s\n",
      "Wall time: 6.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "ModelRandomForestClassifiern = RandomForestClassifier(random_state=0)\n",
    "ModelRandomForestClassifiern.fit(train_tf_idf, train_target)\n",
    "\n",
    "# на валидационных данных F1 =:\n",
    "predictions = ModelRandomForestClassifiern.predict(valid_tf_idf)\n",
    "print(f1_score(predictions, valid_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069651741293531\n"
     ]
    }
   ],
   "source": [
    "# на тестовых данных F1 = :\n",
    "predictions = ModelRandomForestClassifiern.predict(test_tf_idf)\n",
    "print(f1_score(predictions, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборочное сокращение датасета - дало отличный результат. Модель меньше училось, но более точна.\n",
    "\n",
    "скорость LogisticRegression намного быстрее чем у RandomForestClassifier\n",
    "\n",
    "Хотя по метрике качества - она её превосходит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Закомментированный BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.BertTokenizer(vocab_file='/datasets/ds_bert/vocab.txt')\n",
    "\n",
    "#tokenized = data['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "#max_len = 0\n",
    "#for i in tokenized.values:\n",
    "#    if len(i) > max_len:\n",
    "#        max_len = len(i)\n",
    "\n",
    "#padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "#attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "#config = transformers.BertConfig.from_json_file('/datasets/ds_bert/bert_config.json')\n",
    "#model = transformers.BertModel.from_pretrained('/datasets/ds_bert/rubert_model.bin', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size = 100\n",
    "#embeddings = []\n",
    "#for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "#        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "#        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "#        \n",
    "#        with torch.no_grad():\n",
    "#            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "#        \n",
    "#        embeddings.append(batch_embeddings[0][:,0,:].numpy())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = np.concatenate(embeddings)\n",
    "#target = data['toxic']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
